<!DOCTYPE html>
<!--
==============================================================================
           "GitHub HTML5 Pandoc Template" v2.1 — by Tristano Ajmone           
==============================================================================
Copyright © Tristano Ajmone, 2017, MIT License (MIT). Project's home:

- https://github.com/tajmone/pandoc-goodies

The CSS in this template reuses source code taken from the following projects:

- GitHub Markdown CSS: Copyright © Sindre Sorhus, MIT License (MIT):
  https://github.com/sindresorhus/github-markdown-css

- Primer CSS: Copyright © 2016-2017 GitHub Inc., MIT License (MIT):
  http://primercss.io/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The MIT License 

Copyright (c) Tristano Ajmone, 2017 (github.com/tajmone/pandoc-goodies)
Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (sindresorhus.com)
Copyright (c) 2017 GitHub Inc.

"GitHub Pandoc HTML5 Template" is Copyright (c) Tristano Ajmone, 2017, released
under the MIT License (MIT); it contains readaptations of substantial portions
of the following third party softwares:

(1) "GitHub Markdown CSS", Copyright (c) Sindre Sorhus, MIT License (MIT).
(2) "Primer CSS", Copyright (c) 2016 GitHub Inc., MIT License (MIT).

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Julian Thukral, Julien Vitay and Fred Hamker" />
  <title>Forward models in the cerebellum</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title">Forward models in the cerebellum</h1>
<p class="author">Julian Thukral, Julien Vitay and Fred Hamker</p>
</header>
<h1 id="results-report"><span class="header-section-number">1</span> Results Report</h1>
<p>Movements requier precise timing and an aqccurate representation of the bodys state in the enviroment. Yet sensory feedback is subject to variable degrees of delay. With delayed information about the state of the body movement control would occure based on a past state and not the current state, resulting in false corrections for example overshooting the target. It is believed that movement control is based on internal models predicting the future state of the body. The two internal models of motor control are the inverse model, which issues a motor command given the current state and the desired state of the body and the forward model which predicts the future state of the body given the current state and an efference copy of said motor command. The comparison between the predicted and the obtained state results in the prediction error, which again is used in motor control, motor learning and as a key component in generating a Sense of Agency. The Sense of Agency describes the experience of controlling our own actions and through them events in the outside world. Usually we don’t question if we are the agent of our own actions, it only comes into focus by the element of surprise when there is an incongruence of intention and action outcome. The cerebellum is thought to be the locus of the forward model. Acting not only as predictor of movements but also as the comparator between obtained and desired/predicted state. In this work we have built a neural forward model based on the architecture of the cerebellum. The model is trained to predict the future position of the hand of a 2D planar arm going in a circular motion.<br />
Furthermore we tested the capabilities of the model with an experiment based on the Random Dot Task.</p>
<p><br />
<br />
<br />
</p>
<h1 id="methods"><span class="header-section-number">2</span> Methods</h1>
<h2 id="description-of-the-model"><span class="header-section-number">2.1</span> Description of the model</h2>
<p>The proposed network is a reservoir computing model structurally inspired by the human cerebellum. Inputs from the cerebral cortex are fed through a intermediary layer into the reservoir via mossy fibers and to the output cells (projection neurons). A general hebbian algorithm (gha) layer is used as an intermediary step to decorrelate the input information before it is redirected to the reservoir.</p>
<p>The reservoir consists of randomly and recurrently connected neurons. It emulates the recurrent connectivity between granule cells (excitatory) and golgi cells (inhibitory), wich is able to exhibit strong non-linear dynamics.</p>
<p>The activity of the reservoir is read out by a layer of purkinje cells, which in turn inhibit the projection neurons (dentate nucleus neurons). The projection neurons fire rate functions as the model output on which basis the error is calculated. Based on the error feedback from the inferior olive cells synaptic weights are adjusted between the reservoir and the purkinje cell layer. The layers of the purkinje cells, inferior olive cells and the projection neurons consist of each one neuron for the x and y coordinate.</p>
<figure>
<img src="img/cb_model.png" alt="Figure 1: Structure of the model." style="width: 70%; margin: auto; .center" /><figcaption><strong>Figure 1:</strong> Structure of the model.</figcaption>
</figure>
<p>The model is trained to predict the next position of the hand of a 2d arm ((<span class="math inline">\(x_{t+1}\)</span>, <span class="math inline">\(y_{t+1}\)</span>)) based on the current position ((<span class="math inline">\(x_{t}\)</span>, <span class="math inline">\(y_{t}\)</span>)), the previous movement (<span class="math inline">\(\Delta x = x_{t} - x_{t-1}\)</span>; <span class="math inline">\(\Delta y = y_{t} - y_{t-1}\)</span>). and a movement command in form of the <span class="math inline">\(\Delta\)</span> of the joint angles (<span class="math inline">\(\Delta\Theta_{elbow}\)</span> and <span class="math inline">\(\Delta\Theta_{shoulder}\)</span> ). The base of the arm is situated at the coordinate origin.</p>
<p><img src="img/arm.png" alt="Figure 2: 2D arm model. \Theta1 and \Theta2 are the movement commands. Source: doi:10.1109/IRIS.2017.8250090" style="width: 45%; margin: auto; .center" /><br />
<br />
<br />
<img src="img/input_explain.png" alt="Figure 3: Example of Input." style="width: 75%; margin: auto; .center" /></p>
<p>The inputs correspond to positions on the target circle and not to predictions of the model. The current position of the hand at <span class="math inline">\(t+1\)</span> is represented by the target of timestep <span class="math inline">\(t\)</span> and not by the model prediction at <span class="math inline">\(t\)</span>. <span class="math inline">\(\Delta\Theta_{elbow}\)</span> and <span class="math inline">\(\Delta\Theta_{shoulder}\)</span> are calculated as the movement command from the last position on the target circle to the current position on the target circle. The same principle applies to the information about the last step i.e. <span class="math inline">\(\Delta x = x_t - x_{t-1} = x_{target_t-1}-x_{t-1}\)</span>. Keep in mind that <span class="math inline">\(x_t = x_{target_t-1}\)</span>.</p>
<p>The error is calculated as a normalized mean-square error (MSE) based on the difference between the predicted position (<span class="math inline">\(x_{t+1}\)</span>, <span class="math inline">\(y_{t+1}\)</span>) and the target (<span class="math inline">\(x_{target}\)</span>, <span class="math inline">\(y_{target}\)</span>).</p>
<p>Training is done using 25.000 circles, with 8 predictions/steps each. Each circle differentiates in the center of the circle, the radius, and the starting position of the hand in the circle. Each movement per timestep step was set to a movement of the hand of a constant 43 degrees. Thus each circle needed 8 steps for one complete circumnavigation.<br />
<br />
<br />
</p>
<h2 id="equations"><span class="header-section-number">2.2</span> Equations</h2>
<p><br />
</p>
<h3 id="neurons"><span class="header-section-number">2.2.1</span> Neurons</h3>
<p>The firerate of the input neurons was set to the desired input. There were six input neurons representing each of the six different input (x,y, <span class="math inline">\(\Delta\Theta_{elbow}\)</span>, <span class="math inline">\(\Delta\Theta_{shoulder}\)</span>, <span class="math inline">\(\Delta x\)</span>, <span class="math inline">\(\Delta y\)</span>).</p>
<p><br />
</p>
<p>The neurons of the gha, input and purkinje can be described with the following equation:</p>
<p><span class="math display">\[
\begin{aligned}r_{j} = \sum^i w_{ij}\, r_i \\\end{aligned}
\]</span></p>
<p><span class="math inline">\(w_{ij}\)</span> represents the weigths between the current layer and the previous layer, while <span class="math inline">\(r_i\)</span> is the firerate of the previous connected neuron.</p>
<p>In case of the gha layer the weights are pre-trained with the generalized hebbian algorithm and updated throughout the training.</p>
<p>The weights of the purkinje cell layer are initialized according to a normal distribution (<span class="math inline">\(m = 0\)</span> and <span class="math inline">\(\sigma = 0.1\)</span>) and updated each step during training, using a modified delta learning rule.</p>
<p>The firrate rate of the input layer is simply set in python.</p>
<p><br />
</p>
<p>The reservoir neurons follow a first-order ODEs:</p>
<p><span class="math display">\[
    \tau + \frac {dx(t)}{dt} + x(t)= \sum w^{in} \, r^{in}(t)+g
\]</span></p>
<p><span class="math inline">\(\tau = 10\)</span> ensure relativly quick the dynamics in the reservoir and the scaling factor <span class="math inline">\(g = 1\)</span> characterizes the strentgh of the recurrent connections in the reservoir at the lower edge of chaos, a base prequesite for an Echo State Reservoir. The weights <span class="math inline">\(w^{in}\)</span> are set using a random uniform distribution between the <span class="math inline">\(min=-0.5\)</span> and the <span class="math inline">\(max=0.5\)</span>, while <span class="math inline">\(r^{in}\)</span> is given by the firerrate of the gha pre-neuron.<br />
The activation function of the reservoir neurons was given as:</p>
<p><span class="math display">\[ 
r = tanh(x(t))
\]</span></p>
<p><br />
</p>
<p>The firerate of the Inferior olive neurons wich feed the error feedback to the Purkinje Cells is calculated and set in python each step.</p>
<p><br />
<br />
The projection neurons are defined by the similar equation as the prukinje cells: <span class="math display">\[
\begin{aligned}r_{j} = \sum^i w^{in}_{ij}\, I_i  - \sum^i w^{purk}_{ij}\, r_i \\\end{aligned}
\]</span><br />
The projection neurons recieve a copy of the input from the mossy fibres and input from the purkinjie cells. <span class="math inline">\(w^{in}_{ij}\)</span> and <span class="math inline">\(w^{purk}_{ij}\)</span> are set to <span class="math inline">\(1\)</span>. The firerate of the projection neurons equals the copy of the mossy fibre input minus the purkinjie firerate. Hence the purkinjie cells don’t learn to predict the new coordinates of the effector, but the movement between the old coordinates and the new (i.e. <span class="math inline">\(\Delta x = x_{t+1} - x_{t}\)</span> ) .</p>
<p><br />
<br />
<br />
### Synapses<br />
During training learning only occurs in the synapses between the reservoir and the purkinje cell layer and at the gha layer.</p>
<p>The synapses between the input layer and the gha layer were updated using the Sanger’s Rule aka the Generalized Hebbian Algortih (GHA). The purpose of using a GHA is to decorrelate the input, and relaying a clearer input signal to the reservoir.</p>
<p>The GHA was implemented in its matrix form using the following equation: <span class="math display">\[
\begin{aligned}\Delta w_{(t)} =  \eta_{(t)} \, \bigg(y_{(t)} \, x_{(t)}^T - LT[y_{(t)} \, y_{(t)}^T] \, w_{(t)}\bigg) \\\end{aligned}
\]</span><br />
In reservoir computing models learning is based on the wieght adjustment between the reservoir layer and the output layer in case of this model the purkije cell layer. Weights are adjusted with a modified delta learning algorithm:</p>
<p><span class="math display">\[
\begin{aligned}\Delta w_{ij} =  \eta \, (r_{i} \, e_{j} - c \, w_{ij)}) \\\end{aligned}
\]</span><br />
</p>
<p>The learning rate was set to <span class="math inline">\(\eta = 0.005\)</span> . A cost parameter was added and set to <span class="math inline">\(c = 0.001\)</span>. Each step in the circle the error, <span class="math inline">\(e_j\)</span>, was calculated in python as (target - model_prediction) and fed into the prukinje cell layer via the inferior olive cells.<br />
<br />
<br />
</p>
<h2 id="experimental-task"><span class="header-section-number">2.3</span> Experimental Task</h2>
<p>The experimental task was designed to emulate the Random Dot Task. In the random dot task, the movement of the dot displayed on the monitor consists of the movement of the test subject and added noise. The ratio is specified by the control level, which defines the percentage of control a test subject has after the movement of the dot. To simulate a similar task for the model, a visual display circle is first calculated, based on the target circle with added noise. The added noise is taken from the same noise array as in the dot task, an array with prerecorded pseudorandom movements. At each step, a random movement in the array is taken, normalized and added to the movement of the target circle. The control level specifies the percentage of the movement done by noise or true movement.</p>
<p>This visual display is fed as input into the model. i.e.:</p>
<ul>
<li>the current position of the effector ((<span class="math inline">\(x_{t}\)</span>, <span class="math inline">\(y_{t})_{vd}\)</span>) is taken from the visual display and not from the target circle or the prediction of them model of the previous step.</li>
<li><span class="math inline">\(\Delta\Theta_{elbow}\)</span> and <span class="math inline">\(\Delta\Theta_{shoulder}\)</span> are calculated for the movement from (<span class="math inline">\(x_{t}\)</span>, <span class="math inline">\(y_{t})_{vd}\)</span> to the target on the target circle for current step</li>
<li><span class="math inline">\(\Delta x\)</span> and <span class="math inline">\(\Delta y\)</span> are based on the last movement by the visual display (<span class="math inline">\(\Delta x = (x_t - x_{t-1})_{vd}\)</span> ; <span class="math inline">\(\Delta y = (y_t - y_{t-1})_{vd}\)</span> )</li>
</ul>
<p>The condition in which the model is fed with the visual display will be called model agency condition. The error for each circle was calculated as mean square error.<br />
<br />
</p>
<figure>
<img src="img/test_explain.png" alt="Figure 4: Test Input Explanation" style="width: 80%; margin: auto; .center" /><figcaption><strong>Figure 4:</strong> Test Input Explanation</figcaption>
</figure>
<p>Testing was conducted with 1000 trials without learning, starting with control level 0 and incrementing it by +0.001 each run. Each trial consists of 20 circles. Circles were created as in the training phase, differentiating in radius, circle center and starting position on the circle.</p>
<p><br />
<br />
<br />
</p>
<h1 id="results"><span class="header-section-number">3</span> Results</h1>
<p><br />
## Training<br />
As can be seen in Fig. 5, the training MSE decreases rapidly during training reaching and MSE of 0.3177 after 100 circles. after the first few circles. Taking a longer comparibly longer time for optimization in this run The model reached an MSE of 0.0034 after 25.000 circles.<br />
</p>
<figure>
<img src="img/training_plots/training_Circle_0-100.png" alt="Figure 5: Training MSE of the first 100 Training Circles. MSE Circle 100 = 0.3177." style="width: 60%; margin: auto;" /><figcaption><strong>Figure 5:</strong> Training MSE of the first 100 Training Circles. MSE Circle 100 = 0.3177.</figcaption>
</figure>
<p>The following video illustrates how the model performance progresses through training. The quick improvement in the ability to predict the future state can be seen in the closing margin between the target circle (red) and the model predictions (blue). (Note: The Control Level:0001 in the title is to be ignored.)</p>
<p><br />
</p>
<video controls width=60%>
<source src="./videos/training/training_circles_0-25000.mp4"
            type="video/mp4">
</video>
<p><strong>TODO: give more explanations on what should be observed. Julian: Can we talk about this?</strong></p>
<p><br />
<br />
<br />
</p>
<h2 id="experimental-task-1"><span class="header-section-number">3.1</span> Experimental Task</h2>
<p><br />
</p>
<p>Below are examplary videos of a test run. The videos show five examples of the visual display and the movement of the model agency condition at the control levels 0.2, 0.5 and 0.7. The control level and the mse for the circle are displayed in the title.</p>
<p>At control level 0.2 most of the displayed movement of the effector (visual display circle in golden) is given by random noise and not the movement done by the theoretical test subject. Thus the visual display is expected to be quite erratic and the form far removed from the shape of the intended circle. Since the models movement command still aims at the target circle, the model should compensate the erratic movement of the visual display at least partly and draw something closer to the intended circle. With growing control over the movement the visual display gets less erratic and the model is able to compensate the noise better. This can be observed in the three example videos.</p>
<p><br />
</p>
<p><strong>Control level 0.2</strong></p>
<video controls width=60%>
<source src="./videos/test_only_ac_to_vd/test_video_cl_0.2.mp4"
            type="video/mp4">
</video>
<p><br />
</p>
<p><strong>Control level 0.5</strong></p>
<video controls width=60%>
<source src="./videos/test_only_ac_to_vd/test_video_cl_0.5.mp4"
            type="video/mp4">
</video>
<p><br />
</p>
<p><strong>Control level 0.7</strong></p>
<video controls width=60%>
<source src="./videos/test_only_ac_to_vd/test_video_cl_0.7.mp4"
            type="video/mp4">
</video>
<p><br />
<br />
</p>
<p>Fig. 6 depicts the influence of the control level on the MSE between the model agency condition and the visual display. As expected, the MSE increases as the control level decreases.</p>
<figure>
<img src="img/test_plots/ED_from_model_ac_to_vd_cl_0-1.png" alt="Figure 6: Test MSE of Model Agency condition to Visual Display. Upper chart depicts the raw MSE values, while the lower chart smothed the graph with a running mean of 100." style="width: 60%; margin: auto;" /><figcaption><strong>Figure 6:</strong> Test MSE of Model Agency condition to Visual Display. Upper chart depicts the raw MSE values, while the lower chart smothed the graph with a running mean of 100.</figcaption>
</figure>
</article>
</body>
</html>
